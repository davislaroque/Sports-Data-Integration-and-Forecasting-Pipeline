{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d995f54e",
   "metadata": {},
   "source": [
    "# Technology Stocks Signal Forecasting with Machine Learning and Deep Learning\n",
    "\n",
    "This notebook walks through a full mini-pipeline for technology equities:\n",
    "\n",
    "* Download daily prices with **yfinance**.\n",
    "* Engineer features to predict **next-day returns**.\n",
    "* Train multiple models (logistic regression, random forest, and a small **LSTM**).\n",
    "* Generate **buy/sell signals** using predicted probabilities.\n",
    "* Run a simplified **options-style strategy** backtest with transaction costs and slippage.\n",
    "\n",
    "The notebook is heavily commented for students with a background similar to **STAT 453 (deep learning / generative models)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42320c51",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "If you are running this outside the prepared environment, uncomment the installation cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7677e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yfinance tensorflow scikit-learn matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.options.display.float_format = '{:.4f}'.format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a8fb4",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Feel free to tweak the ticker list, date range, and modeling hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14387f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universe of large-cap tech names\n",
    "TICKERS = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\", \"NVDA\", \"TSLA\", \"AMD\", \"AVGO\", \"CRM\"]\n",
    "\n",
    "START_DATE = \"2016-01-01\"\n",
    "END_DATE = None  # None fetches up to the most recent available date\n",
    "\n",
    "# Modeling parameters\n",
    "LOOKBACK_DAYS = 10  # sequence length for the LSTM\n",
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.15\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Trading parameters\n",
    "TRANSACTION_COST_BPS = 10  # 1bp ~ 0.01%\n",
    "SLIPPAGE_BPS = 5  # 0.05% per trade\n",
    "OPTION_PREMIUM = 0.01  # 1% premium cost for the simplified options-style bet\n",
    "CALL_DELTA = 0.6  # sensitivity of payoff to underlying price moves\n",
    "PUT_DELTA = -0.5\n",
    "PROB_BUY_THRESHOLD = 0.55\n",
    "PROB_SELL_THRESHOLD = 0.45\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307e4bc",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "We use `yfinance.download` to pull daily OHLCV data for each ticker. The data are stored in a tidy DataFrame with a ticker column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4acb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(tickers, start, end=None):\n",
    "    '''Download adjusted close data for each ticker and stack into one DataFrame.'''\n",
    "    price_dict = {}\n",
    "    for t in tickers:\n",
    "        df = yf.download(t, start=start, end=end, progress=False)\n",
    "        df = df.rename(columns=lambda c: c.lower())\n",
    "        df['ticker'] = t\n",
    "        price_dict[t] = df\n",
    "    data = pd.concat(price_dict.values()).reset_index().rename(columns={'index': 'date'})\n",
    "    data = data.sort_values(['date', 'ticker']).reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "prices = fetch_data(TICKERS, START_DATE, END_DATE)\n",
    "print(prices.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95636b5b",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We build simple yet intuitive features:\n",
    "\n",
    "* Daily returns and next-day returns (target).\n",
    "* Rolling mean/volatility of returns.\n",
    "* Momentum proxies (rolling max/min, RSI-like feature).\n",
    "* Volume z-score.\n",
    "\n",
    "Each ticker is processed independently and then concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac90c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    feats = []\n",
    "    for t, g in df.groupby('ticker'):\n",
    "        g = g.copy()\n",
    "        g['return'] = g['adj close'].pct_change()\n",
    "        g['next_return'] = g['return'].shift(-1)\n",
    "        g['target'] = (g['next_return'] > 0).astype(int)\n",
    "        g['ret_ma_5'] = g['return'].rolling(5).mean()\n",
    "        g['ret_ma_10'] = g['return'].rolling(10).mean()\n",
    "        g['ret_std_10'] = g['return'].rolling(10).std()\n",
    "        g['high_roll_10'] = g['high'].rolling(10).max() / g['close'] - 1\n",
    "        g['low_roll_10'] = g['low'].rolling(10).min() / g['close'] - 1\n",
    "        g['volume_z'] = (g['volume'] - g['volume'].rolling(20).mean()) / g['volume'].rolling(20).std()\n",
    "        # Simple RSI-style oscillator\n",
    "        up = g['close'].diff().clip(lower=0).rolling(14).mean()\n",
    "        down = -g['close'].diff().clip(upper=0).rolling(14).mean()\n",
    "        rs = up / (down + 1e-9)\n",
    "        g['rsi'] = 100 - 100 / (1 + rs)\n",
    "        g['rsi'] = (g['rsi'] - 50) / 50  # center around zero\n",
    "        g = g.dropna()\n",
    "        feats.append(g)\n",
    "    return pd.concat(feats).reset_index(drop=True)\n",
    "\n",
    "features = engineer_features(prices)\n",
    "print(features[['date', 'ticker', 'return', 'target']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fd795",
   "metadata": {},
   "source": [
    "## Train / Validation / Test Split\n",
    "We keep the chronological order to avoid look-ahead bias. A portion of the most recent data is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_split(df, test_size=0.15, val_size=0.2):\n",
    "    df = df.sort_values('date')\n",
    "    n = len(df)\n",
    "    test_cut = int(n * (1 - test_size))\n",
    "    val_cut = int(test_cut * (1 - val_size))\n",
    "    train = df.iloc[:val_cut]\n",
    "    val = df.iloc[val_cut:test_cut]\n",
    "    test = df.iloc[test_cut:]\n",
    "    return train, val, test\n",
    "\n",
    "train_df, val_df, test_df = time_based_split(features, TEST_SIZE, VAL_SIZE)\n",
    "print(len(train_df), len(val_df), len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db5cd5",
   "metadata": {},
   "source": [
    "## Classical Machine Learning Models\n",
    "We treat each day-ticker observation as a tabular feature vector. Ticker is one-hot encoded to allow models to learn per-name effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS = ['ret_ma_5', 'ret_ma_10', 'ret_std_10', 'high_roll_10', 'low_roll_10', 'volume_z', 'rsi', 'return']\n",
    "\n",
    "# One-hot encode ticker\n",
    "train_X = pd.get_dummies(train_df[['ticker'] + FEATURE_COLS], columns=['ticker'])\n",
    "val_X = pd.get_dummies(val_df[['ticker'] + FEATURE_COLS], columns=['ticker'])\n",
    "test_X = pd.get_dummies(test_df[['ticker'] + FEATURE_COLS], columns=['ticker'])\n",
    "\n",
    "# Align dummy columns\n",
    "train_X, val_X = train_X.align(val_X, join='left', axis=1, fill_value=0)\n",
    "train_X, test_X = train_X.align(test_X, join='left', axis=1, fill_value=0)\n",
    "\n",
    "target_train = train_df['target']\n",
    "target_val = val_df['target']\n",
    "target_test = test_df['target']\n",
    "\n",
    "models = {}\n",
    "\n",
    "log_reg = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=500, class_weight='balanced'))\n",
    "])\n",
    "log_reg.fit(train_X, target_train)\n",
    "models['Logistic Regression'] = log_reg\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_clf.fit(train_X, target_train)\n",
    "models['Random Forest'] = rf_clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4007c",
   "metadata": {},
   "source": [
    "## Sequence Model (LSTM)\n",
    "We build sequences of length `LOOKBACK_DAYS` for each ticker, so the model can learn short-term temporal patterns. Features are standardized per ticker to avoid dominance by high-price names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(df, lookback=10):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for t, g in df.groupby('ticker'):\n",
    "        g = g.sort_values('date')\n",
    "        feat_mat = g[FEATURE_COLS].values\n",
    "        # Standardize per ticker\n",
    "        mean = feat_mat.mean(axis=0, keepdims=True)\n",
    "        std = feat_mat.std(axis=0, keepdims=True) + 1e-9\n",
    "        feat_mat = (feat_mat - mean) / std\n",
    "        for i in range(len(g) - lookback):\n",
    "            sequences.append(feat_mat[i:i+lookback])\n",
    "            labels.append(g['target'].iloc[i+lookback])\n",
    "    return np.array(sequences, dtype=np.float32), np.array(labels, dtype=np.int32)\n",
    "\n",
    "train_seq_X, train_seq_y = build_sequences(train_df, LOOKBACK_DAYS)\n",
    "val_seq_X, val_seq_y = build_sequences(val_df, LOOKBACK_DAYS)\n",
    "test_seq_X, test_seq_y = build_sequences(test_df, LOOKBACK_DAYS)\n",
    "\n",
    "print(\"LSTM train shape:\", train_seq_X.shape)\n",
    "\n",
    "lstm_model = models.Sequential([\n",
    "    layers.Input(shape=(LOOKBACK_DAYS, len(FEATURE_COLS))),\n",
    "    layers.LSTM(32, return_sequences=False),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    train_seq_X, train_seq_y,\n",
    "    validation_data=(val_seq_X, val_seq_y),\n",
    "    epochs=25,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "models['LSTM'] = lstm_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe39ded",
   "metadata": {},
   "source": [
    "## Evaluation Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac71ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, X, y_true, name):\n",
    "    # Handle keras vs scikit-learn predict_proba interface\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        prob = model.predict_proba(X)[:, 1]\n",
    "    else:\n",
    "        prob = model.predict(X).ravel()\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, pred)\n",
    "    auc = roc_auc_score(y_true, prob)\n",
    "    print(f\"\n",
    "{name} Accuracy: {acc:.3f}, ROC-AUC: {auc:.3f}\")\n",
    "    print(classification_report(y_true, pred, digits=3))\n",
    "    return prob\n",
    "\n",
    "probs_test = {}\n",
    "for name, model in models.items():\n",
    "    if name == 'LSTM':\n",
    "        probs_test[name] = model.predict(test_seq_X).ravel()\n",
    "    else:\n",
    "        probs_test[name] = evaluate_classifier(model, test_X, target_test, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ffb38",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "We visualize coefficient magnitudes for logistic regression and mean decrease impurity for the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96834acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, title):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    if hasattr(model, 'coef_'):\n",
    "        imp = np.abs(model.coef_[0])\n",
    "    else:\n",
    "        imp = model.feature_importances_\n",
    "    idx = np.argsort(imp)[::-1][:15]\n",
    "    plt.barh(np.array(feature_names)[idx][::-1], imp[idx][::-1])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(log_reg['clf'], train_X.columns, 'Logistic Regression | |coef|')\n",
    "plot_feature_importance(rf_clf, train_X.columns, 'Random Forest Feature Importance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc47da",
   "metadata": {},
   "source": [
    "## Signal Generation\n",
    "We convert predicted probabilities into discrete trading signals.\n",
    "\n",
    "* **Buy / call-style bet:** probability > `PROB_BUY_THRESHOLD`.\n",
    "* **Sell / put-style bet:** probability < `PROB_SELL_THRESHOLD`.\n",
    "* Otherwise: stay flat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dfef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_signals(df, probs, model_name):\n",
    "    signals = df[['date', 'ticker', 'next_return']].copy()\n",
    "    signals['prob_pos'] = probs\n",
    "    signals['signal'] = 0\n",
    "    signals.loc[signals['prob_pos'] > PROB_BUY_THRESHOLD, 'signal'] = 1\n",
    "    signals.loc[signals['prob_pos'] < PROB_SELL_THRESHOLD, 'signal'] = -1\n",
    "    signals['model'] = model_name\n",
    "    return signals\n",
    "\n",
    "# Align test probabilities to dataframe rows for classical models\n",
    "signals_all = []\n",
    "for name in ['Logistic Regression', 'Random Forest']:\n",
    "    sig = generate_signals(test_df.reset_index(drop=True), probs_test[name], name)\n",
    "    signals_all.append(sig)\n",
    "\n",
    "# LSTM sequences need to be realigned because sequence building drops the first LOOKBACK_DAYS rows per ticker\n",
    "# We rebuild a mapping to the underlying rows that ended each sequence\n",
    "seq_indices = []\n",
    "for t, g in test_df.groupby('ticker'):\n",
    "    g = g.sort_values('date').reset_index(drop=True)\n",
    "    for i in range(len(g) - LOOKBACK_DAYS):\n",
    "        seq_indices.append(g.index[i + LOOKBACK_DAYS])\n",
    "\n",
    "lstm_probs = pd.Series(probs_test['LSTM'], index=seq_indices)\n",
    "lstm_df = test_df.loc[seq_indices].reset_index(drop=True)\n",
    "signals_all.append(generate_signals(lstm_df, lstm_probs.values, 'LSTM'))\n",
    "\n",
    "signals_df = pd.concat(signals_all).reset_index(drop=True)\n",
    "signals_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85e8e9",
   "metadata": {},
   "source": [
    "## Simplified Options-Style Strategy & Backtest\n",
    "We approximate option payoffs using delta times the underlying return minus a fixed premium:\n",
    "\n",
    "* **Call-like (signal = +1):** payoff = max(CALL_DELTA * return - premium, -premium)\n",
    "* **Put-like (signal = -1):** payoff = max(PUT_DELTA * return - premium, -premium)\n",
    "\n",
    "Transaction costs and slippage reduce returns each time a position is opened. We compound P&L equally weighted across tickers each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68955828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_payoff(ret, signal):\n",
    "    if signal == 1:\n",
    "        raw = CALL_DELTA * ret - OPTION_PREMIUM\n",
    "    elif signal == -1:\n",
    "        raw = PUT_DELTA * ret - OPTION_PREMIUM\n",
    "    else:\n",
    "        return 0.0\n",
    "    return max(raw, -OPTION_PREMIUM)  # limited downside to premium\n",
    "\n",
    "\n",
    "def backtest(signals, cost_bps=10, slippage_bps=5):\n",
    "    df = signals.copy()\n",
    "    df['day'] = pd.to_datetime(df['date'])\n",
    "    daily_results = []\n",
    "    for (day, model), g in df.groupby(['day', 'model']):\n",
    "        pnl = 0\n",
    "        n_trades = (g['signal'] != 0).sum()\n",
    "        for _, row in g.iterrows():\n",
    "            pnl += option_payoff(row['next_return'], row['signal'])\n",
    "        # Apply linear costs\n",
    "        total_bps = (cost_bps + slippage_bps) * n_trades\n",
    "        pnl -= total_bps / 10000\n",
    "        daily_results.append({'date': day, 'model': model, 'pnl': pnl})\n",
    "    res = pd.DataFrame(daily_results).sort_values('date')\n",
    "    res['equity'] = (1 + res['pnl']).groupby(res['model']).cumprod()\n",
    "    return res\n",
    "\n",
    "bt_results = backtest(signals_df, TRANSACTION_COST_BPS, SLIPPAGE_BPS)\n",
    "bt_results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b14f0",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "We visualize equity curves and summarize key metrics like cumulative return and hit rate (fraction of profitable days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4883343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_backtest(bt_df):\n",
    "    summaries = []\n",
    "    for model, g in bt_df.groupby('model'):\n",
    "        cum_return = g['equity'].iloc[-1] - 1\n",
    "        hit_rate = (g['pnl'] > 0).mean()\n",
    "        vol = g['pnl'].std() * np.sqrt(252)\n",
    "        sharpe = (g['pnl'].mean() * 252) / (vol + 1e-9)\n",
    "        summaries.append({\n",
    "            'Model': model,\n",
    "            'Cumulative Return': cum_return,\n",
    "            'Hit Rate': hit_rate,\n",
    "            'Sharpe (naive)': sharpe\n",
    "        })\n",
    "    return pd.DataFrame(summaries)\n",
    "\n",
    "summary_df = summarize_backtest(bt_results)\n",
    "summary_df\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for model, g in bt_results.groupby('model'):\n",
    "    plt.plot(g['date'], g['equity'], label=model)\n",
    "plt.legend()\n",
    "plt.title('Equity Curves')\n",
    "plt.ylabel('Equity (start=1)')\n",
    "plt.xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a0374",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "* Tune thresholds and premium assumptions.\n",
    "* Experiment with richer features (macro signals, volatility indices).\n",
    "* Try other sequence architectures (1D CNNs, Transformers) or calibration methods for probabilities.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
